#  KONTROL: Blueprint de Agentes Cognitivos y Arquitectura RAG (AI/ML Lead)

## 1. Arquitectura de Agentes (LangGraph & Tool Use)

Los agentes de KONTROL son **Cognitivos**, no solo conversacionales. Utilizan un *framework* de **LangGraph** para la orquestaci贸n, lo que les permite realizar **Tool Use** (llamar a nuestros microservicios internos) para tareas complejas.

| Agente | Funci贸n Primaria (IA) | Tool Use (APIs internas que invoca) |
| :--- | :--- | :--- |
| **Tax Optimizer** | An谩lisis contextual de P&L y reglas fiscales. | `ClickHouse.get_projected_pnl()`, `RAG.query_tax_rules_by_jurisdiction()` |
| **Legal/Compliance** | Generaci贸n de documentaci贸n legal y flujos de cumplimiento. | `TaxEngine.generate_audit_trail()`, `ZKEngine.request_zk_proof()`, `Postgres.fetch_kyc_data()` |
| **Security Agent** | An谩lisis de riesgo on-chain y monitoreo de carteras. | `Neo4j.run_risk_score_algorithm()`, `IngestionService.scan_address()` |
| **Transaction Agent** | Interfaz conversacional y *query language* para la data. | `Neo4j.get_tx_path()`, `ClickHouse.query_data_by_hash()` |

## 2. Dise帽o RAG (Retrieval Augmented Generation)

La precisi贸n de los agentes depende de una base de conocimiento (KB) bien segmentada y referenciada.

* **KB 1: Contexto Normativo (Vector Store 1):** Documentaci贸n legal por jurisdicci贸n (leyes fiscales, modelos de renta, circulares bancarias). **Tecnolog铆a:** **Chroma** o **Pinecone**.
* **KB 2: Contexto Transaccional (Vector Store 2):** Embeddings de todas las TXs can贸nicas del usuario. **Tecnolog铆a:** **Redis Vector Store** (por su velocidad).

**Flujo de Query (Ej. Tax Optimizer):** El agente consulta simult谩neamente la **Ley Fiscal** aplicable (KB 1) y el **P&L actual del usuario** (KB 2) para generar una sugerencia precisa y justificada.

## 3. Estrategia de LLM y Manejo de Contexto

* **LLM Choice:** Uso de modelos avanzados (*GPT-4o/Claude 3.5*) bajo licencias *Enterprise* (GCP Vertex AI/Azure AI) para garantizar la **seguridad de los datos** y la **latencia predecible**.
* **Memoria:** Uso de **Redis Cache** para almacenar la memoria de las conversaciones o *jobs* largos de reconciliaci贸n, minimizando las llamadas a la API del LLM para tareas repetitivas.