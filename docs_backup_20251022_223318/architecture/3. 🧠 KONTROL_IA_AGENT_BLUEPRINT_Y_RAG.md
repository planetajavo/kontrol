# 🧠 KONTROL: Blueprint de Agentes Cognitivos y Arquitectura RAG (AI/ML Lead)

## 1. Arquitectura de Agentes (LangGraph & Tool Use)

Los agentes de KONTROL son **Cognitivos**, no solo conversacionales. Utilizan un *framework* de **LangGraph** para la orquestación, lo que les permite realizar **Tool Use** (llamar a nuestros microservicios internos) para tareas complejas.

| Agente | Función Primaria (IA) | Tool Use (APIs internas que invoca) |
| :--- | :--- | :--- |
| **Tax Optimizer** | Análisis contextual de P&L y reglas fiscales. | `ClickHouse.get_projected_pnl()`, `RAG.query_tax_rules_by_jurisdiction()` |
| **Legal/Compliance** | Generación de documentación legal y flujos de cumplimiento. | `TaxEngine.generate_audit_trail()`, `ZKEngine.request_zk_proof()`, `Postgres.fetch_kyc_data()` |
| **Security Agent** | Análisis de riesgo on-chain y monitoreo de carteras. | `Neo4j.run_risk_score_algorithm()`, `IngestionService.scan_address()` |
| **Transaction Agent** | Interfaz conversacional y *query language* para la data. | `Neo4j.get_tx_path()`, `ClickHouse.query_data_by_hash()` |

## 2. Diseño RAG (Retrieval Augmented Generation)

La precisión de los agentes depende de una base de conocimiento (KB) bien segmentada y referenciada.

* **KB 1: Contexto Normativo (Vector Store 1):** Documentación legal por jurisdicción (leyes fiscales, modelos de renta, circulares bancarias). **Tecnología:** **Chroma** o **Pinecone**.
* **KB 2: Contexto Transaccional (Vector Store 2):** Embeddings de todas las TXs canónicas del usuario. **Tecnología:** **Redis Vector Store** (por su velocidad).

**Flujo de Query (Ej. Tax Optimizer):** El agente consulta simultáneamente la **Ley Fiscal** aplicable (KB 1) y el **P&L actual del usuario** (KB 2) para generar una sugerencia precisa y justificada.

## 3. Estrategia de LLM y Manejo de Contexto

* **LLM Choice:** Uso de modelos avanzados (*GPT-4o/Claude 3.5*) bajo licencias *Enterprise* (GCP Vertex AI/Azure AI) para garantizar la **seguridad de los datos** y la **latencia predecible**.
* **Memoria:** Uso de **Redis Cache** para almacenar la memoria de las conversaciones o *jobs* largos de reconciliación, minimizando las llamadas a la API del LLM para tareas repetitivas.